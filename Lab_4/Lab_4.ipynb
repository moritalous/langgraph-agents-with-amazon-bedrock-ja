{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# ラボ 4: 永続性とストリーミング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b724f8",
   "metadata": {},
   "source": [
    "## 環境設定\n",
    "\n",
    "まず、エージェント環境を確立します。このプロセスには、必要な環境変数の読み込み、必要なモジュールのインポート、Tavily 検索ツールの初期化、エージェント状態の定義、そして最後にエージェントの構築が含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import pprint\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "# import local modules\n",
    "dir_current = os.path.abspath(\"\")\n",
    "dir_parent = os.path.dirname(dir_current)\n",
    "if dir_parent not in sys.path:\n",
    "    sys.path.append(dir_parent)\n",
    "from utils import utils\n",
    "\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Set basic configs\n",
    "logger = utils.set_logger()\n",
    "pp = utils.set_pretty_printer()\n",
    "\n",
    "# Load environment variables from .env file or Secret Manager\n",
    "_ = load_dotenv(\"../.env\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "tavily_ai_api_key = utils.get_tavily_api(\"TAVILY_API_KEY\", aws_region)\n",
    "\n",
    "# Set bedrock configs\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Create a bedrock runtime client\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\", region_name=aws_region, config=bedrock_config\n",
    ")\n",
    "\n",
    "# Create a bedrock client to check available models\n",
    "bedrock = boto3.client(\"bedrock\", region_name=aws_region, config=bedrock_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c989adc7",
   "metadata": {},
   "source": [
    "## 永続性の実装\n",
    "\n",
    "次に、永続性の実装に注目します。これを実現するために、LangGraph にチェックポインタの概念を導入します。チェックポインタの機能は、エージェントの処理グラフ内の各ノードの後と各ノード間で状態スナップショットを作成することです。\n",
    "\n",
    "#リソース LangGraph の機能と使用方法をより包括的に理解するには、公式の LangGraph ドキュメントを参照してください。\n",
    "\n",
    "この実装では、チェックポインタとして SQLite セーバーを使用します。この軽量ソリューションは、組み込みデータベース エンジンである SQLite を活用します。このデモではメモリ内データベースを使用していますが、実稼働環境の外部データベースに接続するために簡単に適応できることに留意してください。LangGraph は、より堅牢なデータベース システムを必要とするシナリオ向けに、Redis や Postgres などの他の永続性ソリューションもサポートしています。\n",
    "\n",
    "チェックポインタを初期化した後、`graph.compile` メソッドに渡します。エージェントを拡張して、メモリ オブジェクトに設定する `checkpointer` パラメータを受け入れるようにしました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01441e5e",
   "metadata": {},
   "source": [
    "## エージェント クラス: 詳細な調査\n",
    "\n",
    "`Agent` クラスは実装の要として機能し、言語モデル (Claude)、ツール (Tavily 検索など)、および全体的な会話フローの間のやり取りを調整します。その主要なコンポーネントを調べてみましょう。\n",
    "\n",
    "1. `__init__` メソッド: この初期化子は、モデル、ツール、チェックポインター、およびオプションのシステム メッセージを使用してエージェントを設定します。エージェントの動作を定義する状態グラフを構築します。\n",
    "\n",
    "2. `call_bedrock` メソッド: このメソッドは、Amazon Bedrock を介して Claude モデルを呼び出す役割を担います。現在の状態 (メッセージ) を処理し、モデルの応答を返します。\n",
    "\n",
    "3. `exists_action` メソッド: このメソッドは、モデルからの最新のメッセージにツール呼び出し (実行するアクション) が含まれているかどうかを評価します。\n",
    "\n",
    "4. `take_action` メソッド: このメソッドは、モデルによって指定されたツール呼び出しを実行し、結果を返します。\n",
    "\n",
    "`Agent` クラスは `StateGraph` を使用して会話フローを管理し、明確で管理しやすい構造を維持しながら複雑なインタラクションを可能にします。この設計選択により、永続性とストリーミング機能の実装が容易になります。\n",
    "\n",
    "## ストリーミングの実装\n",
    "\n",
    "エージェントが構成されたので、ストリーミング機能を実装できます。考慮すべきストリーミングの主な側面は 2 つあります。\n",
    "\n",
    "1. メッセージ ストリーミング: 次のアクションを決定する AI メッセージや、アクションの結果を表す観察メッセージなど、個々のメッセージをストリーミングします。\n",
    "\n",
    "2. トークン ストリーミング: 言語モデルの応答の各トークンが生成されるたびにストリーミングします。\n",
    "\n",
    "まず、メッセージ ストリーミングを実装します。人間のメッセージ (「サンフランシスコの天気はどうですか?」など) を作成し、スレッド構成を導入します。このスレッド構成は、永続的なチェックポインター内で複数の会話を同時に管理するために重要であり、複数のユーザーにサービスを提供する実稼働アプリケーションに不可欠です。\n",
    "\n",
    "`invoke` ではなく `stream` メソッドを使用してグラフを呼び出し、メッセージ ディクショナリとスレッド構成を渡します。これにより、状態へのリアルタイム更新を表すイベント ストリームが返されます。\n",
    "\n",
    "実行すると、結果のストリームが表示されます。最初は、実行するアクションを決定する Claude からの AI メッセージ、次に Tavily 検索結果を含むツール メッセージ、最後に最初のクエリに回答する Claude からの別の AI メッセージです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_bedrock)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", self.exists_action, {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_bedrock(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(\n",
    "                ToolMessage(tool_call_id=t[\"id\"], name=t[\"name\"], content=str(result))\n",
    "            )\n",
    "        print(\"Back to the model!\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eafff51",
   "metadata": {},
   "source": [
    "> 追記　プロンプトの翻訳\n",
    "\n",
    "```\n",
    "あなたは賢いリサーチ アシスタントです。検索エンジンを使用して情報を検索してください。\\\n",
    "複数の通話 (同時にまたは連続して) を行うことができます。\\\n",
    "必要な情報がわかっている場合にのみ情報を検索してください。\\\n",
    "フォローアップの質問をする前に情報を検索する必要がある場合は、検索することができます。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f625b",
   "metadata": {},
   "source": [
    "## 永続性のデモンストレーション\n",
    "\n",
    "永続性の実装の有効性を示すために、会話を続け、「LA はどうですか?」というフォローアップの質問を行います。同じスレッド ID を使用することで、前回のやり取りからの連続性が確保されます。Claude はコンテキストを維持し、チェックポイント システムによって提供される永続性により、私たちがまだ天候について尋ねていることを理解します。\n",
    "\n",
    "スレッド ID を変更して「どちらが暖かいですか?」という質問を行うことで、スレッド ID の重要性をさらに強調できます。元のスレッド ID を使用すると、Claude は正確に温度を比較できます。ただし、スレッド ID を変更すると、会話履歴にアクセスできなくなるため、Claude はコンテキストを失います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607bb30",
   "metadata": {},
   "source": [
    "## トークン レベルのストリーミング\n",
    "\n",
    "ストリーミングをより細かく行うために、`astream_events` メソッドを使用してトークン レベルの更新を実装します。この非同期メソッドには非同期チェックポイントが必要であり、これを `AsyncSqliteSaver` を使用して実装します。\n",
    "\n",
    "非同期プログラミングにより、アプリケーションはメイン実行スレッドをブロックすることなく、複数の操作を同時に処理できます。AI モデルからのトークンのストリーミングのコンテキストでは、これはトークンが生成されるときに処理および表示することになり、ユーザー エクスペリエンスの応答性が向上します。`astream_events` メソッドは、この非同期アプローチを利用して、Claude からのトークン レベルの更新を効率的にストリーミングします。\n",
    "\n",
    "新しいスレッド ID で新しい会話を開始し、イベントを反復処理します。具体的には、\"on_chat_model_stream\" タイプのイベントを探します。これらのイベントに遭遇すると、コンテンツを抽出して表示します。\n",
    "\n",
    "実行すると、トークンのストリーミングがリアルタイムで観察されます。 Claude が関数 (ストリーミング可能なコンテンツは生成しません) を呼び出し、その後に最終応答がトークンごとにストリーミングされることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "# # If you are using a newer version of LangGraph, the package was separated:\n",
    "# # !pip install langgraph-checkpoint-sqlite\n",
    "\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "# from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(\"checkpoints.db\") as memory:\n",
    "    abot = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "    messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    async for event in abot.graph.astream_events(\n",
    "        {\"messages\": messages}, thread, version=\"v1\"\n",
    "    ):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Empty content in the context of Amazon Bedrock means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "                print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8e0e9e",
   "metadata": {},
   "source": [
    "## 結論\n",
    "\n",
    "このラボでは、Amazon Bedrock 上の Anthropic の Claude モデルを使用して、永続性とストリーミングの実装について包括的に調査しました。これらの概念は簡単に実装できますが、本番環境レベルの AI アプリケーションを構築するための強力な機能を提供します。\n",
    "\n",
    "複数の同時会話を管理する機能と、会話再開用の堅牢なメモリ システムを組み合わせることは、スケーラブルな AI ソリューションにとって不可欠です。さらに、最終トークンと中間メッセージの両方をストリーミングする機能により、AI の意思決定プロセスに対する比類のない可視性が提供されます。\n",
    "\n",
    "永続性は、人間が関与するインタラクションを可能にする上でも重要な役割を果たします。このトピックについては、次のラボでさらに詳しく調査します。\n",
    "\n",
    "これらの概念の実際的な意味をより深く理解するには、本番環境の AI アプリケーションにおける永続性とストリーミングの実際のケース スタディを調べることをお勧めします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df424a98",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-dev-env",
   "language": "python",
   "name": "agents-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
