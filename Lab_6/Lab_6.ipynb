{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911b3b37-3b29-4833-94f2-bfe47af00c83",
   "metadata": {},
   "source": [
    "# ラボ 6: エッセイ ライター\n",
    "\n",
    "## セットアップとインポート\n",
    "\n",
    "このセクションでは、より複雑なプロジェクトである AI エッセイ ライターを構築します。まず、環境をセットアップして必要なライブラリをインポートします。Amazon Bedrock と Anthropic の Claude モデルを使用しているため、インポートはそれを反映しています。\n",
    "ログ記録をセットアップし、Bedrock を構成し、Tavily API キーを取得します。Tavily は、エッセイの情報を収集するために使用する調査ツールです。Tavily API キーが安全に保存されていることを確認してください。\n",
    "\n",
    "最後に構築する UI の要件は次のとおりです。\n",
    "\n",
    "1. tavily ai キーを `.env` ファイルに追加します。\n",
    "\n",
    "2. LangGraph 0.0.53 で実行していることに注意してください。\n",
    "\n",
    "3. CLI から helper.py を実行し、Web ブラウザーを開いてグラフをステップ実行するか、ノートブックで直接実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "import json, re\n",
    "import pprint\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "# import local modules\n",
    "dir_current = os.path.abspath(\"\")\n",
    "dir_parent = os.path.dirname(dir_current)\n",
    "if dir_parent not in sys.path:\n",
    "    sys.path.append(dir_parent)\n",
    "from utils import utils\n",
    "\n",
    "# Set basic configs\n",
    "logger = utils.set_logger()\n",
    "pp = utils.set_pretty_printer()\n",
    "\n",
    "# Load environment variables from .env file or Secret Manager\n",
    "_ = load_dotenv(\"../.env\")\n",
    "aws_region = os.getenv(\"AWS_REGION\")\n",
    "tavily_ai_api_key = utils.get_tavily_api(\"TAVILY_API_KEY\", aws_region)\n",
    "\n",
    "# Set bedrock configs\n",
    "bedrock_config = Config(\n",
    "    connect_timeout=120, read_timeout=120, retries={\"max_attempts\": 0}\n",
    ")\n",
    "\n",
    "# Create a bedrock runtime client\n",
    "bedrock_rt = boto3.client(\n",
    "    \"bedrock-runtime\", region_name=aws_region, config=bedrock_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ba3e5",
   "metadata": {},
   "source": [
    "## エージェントの状態の定義\n",
    "\n",
    "では、エージェントの状態を定義しましょう。これは、エッセイ ライターに複数のステップがあり、さまざまな情報を追跡する必要があるため、以前のレッスンよりも複雑です。\n",
    "\n",
    "AgentState という TypedDict を作成します。これには次のものが含まれます:\n",
    "\n",
    "- task: エッセイのトピックまたは質問\n",
    "- plan: エッセイの概要\n",
    "- draft: エッセイの現在のバージョン\n",
    "- critique: 現在のドラフトに対するフィードバック\n",
    "- content: Tavily からの調査情報\n",
    "- revision_number: 行った修正の数\n",
    "- max_revisions: 行う修正の最大数\n",
    "\n",
    "これらの要素は、エッセイ作成プロセスを管理し、修正を停止するタイミングを知るのに役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    AnyMessage,\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    ChatMessage,\n",
    ")\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6aac3dc",
   "metadata": {},
   "source": [
    "## モデルの設定\n",
    "\n",
    "Amazon Bedrock 経由で Anthropic の Claude モデルを使用しています。より一貫した出力を得るために、温度を 0 に設定しています。使用しているモデルは claude-3-haiku で、このタスクに適しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    client=bedrock_rt,\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61f1e5",
   "metadata": {},
   "source": [
    "## プロンプトの定義\n",
    "\n",
    "当社のエッセイ ライターは、プロセスのさまざまな段階で複数のプロンプトを使用します。\n",
    "\n",
    "1. **PLAN_PROMPT**: これは、モデルにエッセイのアウトラインを作成するように指示します。\n",
    "\n",
    "2. **WRITER_PROMPT**: これは、計画と調査に基づいてモデルがエッセイを書くようにガイドします。\n",
    "\n",
    "3. **REFLECTION_PROMPT**: これは、モデルにエッセイを批評する方法を伝えます。\n",
    "\n",
    "4. **RESEARCH_PLAN_PROMPT** および **RESEARCH_CRITIQUE_PROMPT**: これらは、調査ステップの検索クエリを生成するのに役立ちます。\n",
    "\n",
    "各プロンプトは、エッセイ作成プロセス内でモデルが特定のタスクを実行するようにガイドするために慎重に作成されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 74
   },
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e07cc4",
   "metadata": {},
   "source": [
    "> 追記　プロンプトの翻訳（PLAN_PROMPT）\n",
    "\n",
    "```\n",
    "あなたは、エッセイの高レベルのアウトラインを書くことを任された専門のライターです。\\\n",
    "ユーザーから提供されたトピックについて、そのようなアウトラインを書いてください。エッセイのアウトラインを、関連するメモやセクションの指示とともに提供してください。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "<content>\n",
    "{content}\n",
    "</content>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c0cee",
   "metadata": {},
   "source": [
    "> 追記　プロンプトの翻訳（WRITER_PROMPT）\n",
    "\n",
    "```\n",
    "あなたは、優れた 5 段落のエッセイを書くことを任されたエッセイ アシスタントです。\\\n",
    "ユーザーのリクエストと最初のアウトラインに合わせて、可能な限り最高のエッセイを作成します。\\\n",
    "ユーザーから批判があった場合は、以前の試みの修正版で応答します。\\\n",
    "必要に応じて、以下のすべての情報を活用してください。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5752",
   "metadata": {},
   "source": [
    "> 追記　プロンプトの翻訳（REFLECTION_PROMPT）\n",
    "\n",
    "```\n",
    "あなたはエッセイの提出物を採点する教師です。\\\n",
    "ユーザーの提出物に対する批評と推奨事項を作成します。\\\n",
    "長さ、深さ、スタイルなどのリクエストを含む詳細な推奨事項を提供します。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4721a3a",
   "metadata": {},
   "source": [
    "> 追記　プロンプトの翻訳（RESEARCH_PLAN_PROMPT）\n",
    "\n",
    "```\n",
    "あなたは、次のエッセイを書くときに使用できる情報を提供する責任を負っている研究者です。\n",
    "関連する情報を収集する検索クエリのリストを生成してください。\n",
    "最大 3 つのクエリのみ生成してください。\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6edcb7",
   "metadata": {},
   "source": [
    "> 追記　プロンプトの翻訳（RESEARCH_CRITIQUE_PROMPT）\n",
    "\n",
    "```\n",
    "あなたは、要求された修正を行う際に使用できる情報を提供する責任を負っている研究者です (以下に概説)。 \n",
    "関連する情報を収集する検索クエリのリストを生成します。生成するクエリは最大 3 つだけです。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c562",
   "metadata": {},
   "source": [
    "### Anthropic モデルによる構造化された出力生成に関する考察:\n",
    "\n",
    "上記のプロンプトを見てください。\n",
    "\n",
    "- これらは Anthropic のプロンプト ガイドに従っていますか?\n",
    "- たとえば REFLECTION_PROMPT の場合、次のような回答構造を要求することで、より一貫性のある出力が得られると思いますか:\n",
    "\n",
    "```xml\n",
    "<answer>\n",
    "  <overall_assessment>\n",
    "    <strengths>\n",
    "      <strength_point></strength_point>\n",
    "      <strength_point></strength_point>\n",
    "    </strengths>\n",
    "    <weaknesses>\n",
    "      <weakness_point></weakness_point>\n",
    "      <weakness_point></weakness_point>\n",
    "    </weaknesses>\n",
    "  </overall_assessment>\n",
    "\n",
    "...\n",
    "\n",
    "  <style_and_language>\n",
    "    <clarity>\n",
    "      <comment></comment>\n",
    "      <recommendation></recommendation>\n",
    "    </clarity>\n",
    "    <tone>\n",
    "      <comment></comment>\n",
    "      <recommendation></recommendation>\n",
    "    </tone>\n",
    "    <grammar_and_mechanics>\n",
    "      <comment></comment>\n",
    "      <recommendation></recommendation>\n",
    "    </grammar_and_mechanics>\n",
    "  </style_and_language>\n",
    "\n",
    "  <length_assessment>\n",
    "    <comment></comment>\n",
    "    <recommendation></recommendation>\n",
    "  </length_assessment>\n",
    "\n",
    "  <conclusion>\n",
    "    <overall_recommendation></overall_recommendation>\n",
    "    <priority_improvements>\n",
    "      <improvement></improvement>\n",
    "      <improvement></improvement>\n",
    "    </priority_improvements>\n",
    "  </conclusion>\n",
    "</answer>\n",
    "```\n",
    "\n",
    "- このような構造の利点と欠点は何でしょうか?\n",
    "- 追加のトークンに価値があるかどうか自問してみてください。詳細でトークンを多用するプロンプトと、より自由形式のプロンプトのどちらに投資すべきでしょうか?\n",
    "\n",
    "- この出力をどのように解析しますか?\n",
    "\n",
    "**ヒント:**\n",
    "LangChain と PyDantic モデルの XMLOutput-Parser を組み合わせることができます。\n",
    "\n",
    "参考までに、langchain に最近追加された `.with_structured_output(...)` などのメソッドに依存せずに XMLOutput パーサーを使用する方法を以下で説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff3ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# Create the XMLOutputParser with our Pydantic model\n",
    "essay_critique_parser = XMLOutputParser()\n",
    "\n",
    "# Example usage\n",
    "xml_string = \"\"\"\n",
    "<answer>\n",
    "  <overall_assessment>\n",
    "    <strengths>\n",
    "      <strength_point>Clear thesis statement</strength_point>\n",
    "      <strength_point>Well-structured paragraphs</strength_point>\n",
    "    </strengths>\n",
    "    <weaknesses>\n",
    "      <weakness_point>Lack of detailed examples</weakness_point>\n",
    "      <weakness_point>Some grammatical errors</weakness_point>\n",
    "    </weaknesses>\n",
    "  </overall_assessment>\n",
    "  <content_evaluation>\n",
    "    <depth_of_analysis>\n",
    "      <comment>The analysis lacks depth in some areas.</comment>\n",
    "      <recommendation>Expand on key points with more detailed explanations.</recommendation>\n",
    "    </depth_of_analysis>\n",
    "    <argument_quality>\n",
    "      <comment>Arguments are logical but could be stronger.</comment>\n",
    "      <recommendation>Provide more evidence to support your claims.</recommendation>\n",
    "    </argument_quality>\n",
    "    <evidence_use>\n",
    "      <comment>Limited use of supporting evidence.</comment>\n",
    "      <recommendation>Incorporate more relevant examples and data.</recommendation>\n",
    "    </evidence_use>\n",
    "  </content_evaluation>\n",
    "  <structure_and_organization>\n",
    "    <comment>The essay has a clear structure but transitions could be improved.</comment>\n",
    "    <recommendation>Work on smoother transitions between paragraphs.</recommendation>\n",
    "  </structure_and_organization>\n",
    "  <style_and_language>\n",
    "    <clarity>\n",
    "      <comment>Writing is generally clear but some sentences are convoluted.</comment>\n",
    "      <recommendation>Simplify complex sentences for better readability.</recommendation>\n",
    "    </clarity>\n",
    "    <tone>\n",
    "      <comment>The tone is appropriate for an academic essay.</comment>\n",
    "      <recommendation>Maintain this formal tone throughout.</recommendation>\n",
    "    </tone>\n",
    "    <grammar_and_mechanics>\n",
    "      <comment>There are a few grammatical errors and typos.</comment>\n",
    "      <recommendation>Proofread carefully to eliminate these errors.</recommendation>\n",
    "    </grammar_and_mechanics>\n",
    "  </style_and_language>\n",
    "  <length_assessment>\n",
    "    <comment>The essay meets the required length.</comment>\n",
    "    <recommendation>No changes needed in terms of length.</recommendation>\n",
    "  </length_assessment>\n",
    "  <conclusion>\n",
    "    <overall_recommendation>This is a solid essay that could be improved with more depth and better proofreading.</overall_recommendation>\n",
    "    <priority_improvements>\n",
    "      <improvement>Deepen analysis with more detailed explanations and examples.</improvement>\n",
    "      <improvement>Carefully proofread to eliminate grammatical errors and typos.</improvement>\n",
    "    </priority_improvements>\n",
    "  </conclusion>\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "# Parse the XML string\n",
    "parsed_critique = essay_critique_parser.parse(xml_string)\n",
    "parsed_critique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d75275",
   "metadata": {},
   "source": [
    "ただし、必要な答えを得るために `.with_structured_output` を使用することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cefa30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class StructuredOutput(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the response\")\n",
    "    content: str = Field(..., description=\"The main content of the response\")\n",
    "    summary: str = Field(..., description=\"A brief summary of the content\")\n",
    "\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0},\n",
    ")\n",
    "\n",
    "structured_llm = llm.with_structured_output(StructuredOutput)\n",
    "\n",
    "response = structured_llm.invoke(\"Tell me about artificial intelligence\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(\n",
    "    StructuredOutput, method=\"xml_mode\"\n",
    ")  # try xml_mode\n",
    "response = structured_llm.invoke(\"Tell me about artificial intelligence\")\n",
    "print(f\"The title:\\t{response.title}\\n\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfafd4a",
   "metadata": {},
   "source": [
    "### Tavily クライアントの設定\n",
    "\n",
    "私たちはリサーチに Tavily API を使用しています。TavilyClient をインポートし、API キーで初期化します。これにより、Web 検索を実行してエッセイの情報を収集できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "\n",
    "tavily = TavilyClient(api_key=tavily_ai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4114e17",
   "metadata": {},
   "source": [
    "## ノード関数の定義\n",
    "\n",
    "ここで、エッセイ執筆プロセスの個々のコンポーネントを作成します。各関数はグラフ内のノードを表します。\n",
    "\n",
    "1. plan_node: エッセイのアウトラインを作成します。\n",
    "\n",
    "2. research_plan_node: 検索クエリを生成し、計画に基づいて情報を取得します。\n",
    "\n",
    "3. generation_node: エッセイの下書きを作成します。\n",
    "\n",
    "4. reflection_node: 現在の下書きを批評します。\n",
    "\n",
    "5. research_critique_node: 批評に基づいて追加の調査を実行します。\n",
    "\n",
    "6. should_continue: 改訂を続行するか停止するかを決定します。\n",
    "\n",
    "これらの各関数は Claude モデルと対話し、それに応じてエージェントの状態を更新します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [SystemMessage(content=PLAN_PROMPT), HumanMessage(content=state[\"task\"])]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(description=\"List of research queries\")\n",
    "\n",
    "\n",
    "def research_plan_node(state: AgentState):\n",
    "    # Set up the Pydantic output parser\n",
    "    parser = PydanticOutputParser(pydantic_object=Queries)\n",
    "\n",
    "    # Create a prompt template with format instructions\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Generate research queries based on the given task.\\n{format_instructions}\\nTask: {task}\\n\",\n",
    "        input_variables=[\"task\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Use the model with the new prompt and parser\n",
    "    queries_output = model.invoke(prompt.format_prompt(task=state[\"task\"]))\n",
    "\n",
    "    # Extract the content from the AIMessage\n",
    "    queries_text = queries_output.content\n",
    "\n",
    "    # Extract the JSON string from the content\n",
    "    json_start = queries_text.find(\"{\")\n",
    "    json_end = queries_text.rfind(\"}\") + 1\n",
    "    json_str = queries_text[json_start:json_end]\n",
    "\n",
    "    # Parse the JSON string\n",
    "    queries_dict = json.loads(json_str)\n",
    "\n",
    "    # Create a Queries object from the parsed JSON\n",
    "    parsed_queries = Queries(**queries_dict)\n",
    "\n",
    "    content = state[\"content\"] or []\n",
    "    for q in parsed_queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f303b1-a4d0-408c-8cc0-515ff980717f",
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state[\"content\"] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\"\n",
    "    )\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4dcb93-6298-4cfd-b3ce-61dfac7fb35f",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state[\"draft\"]),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27526845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    # Set up the Pydantic output parser\n",
    "    parser = PydanticOutputParser(pydantic_object=Queries)\n",
    "\n",
    "    # Create a prompt template with format instructions\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"Generate research queries based on the given critique.\\n{format_instructions}\\nCritique: {critique}\\n\",\n",
    "        input_variables=[\"critique\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    "    )\n",
    "\n",
    "    # Use the model with the new prompt and parser\n",
    "    queries_output = model.invoke(prompt.format_prompt(critique=state[\"critique\"]))\n",
    "\n",
    "    # Extract the content from the AIMessage\n",
    "    queries_text = queries_output.content\n",
    "\n",
    "    # Extract the JSON string from the content\n",
    "    json_start = queries_text.find(\"{\")\n",
    "    json_end = queries_text.rfind(\"}\") + 1\n",
    "    json_str = queries_text[json_start:json_end]\n",
    "\n",
    "    # Parse the JSON string\n",
    "    queries_dict = json.loads(json_str)\n",
    "\n",
    "    # Create a Queries object from the parsed JSON\n",
    "    parsed_queries = Queries(**queries_dict)\n",
    "\n",
    "    content = state[\"content\"] or []\n",
    "    for q in parsed_queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response[\"results\"]:\n",
    "            content.append(r[\"content\"])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff362f49-dcf1-4ea1-a86c-e516e9ab897d",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7044a487",
   "metadata": {},
   "source": [
    "## グラフの作成\n",
    "\n",
    "ノードが定義されたので、グラフを作成できます。LangGraph の StateGraph を使用して、エッセイ作成プロセスのフローを作成します。各ノードをグラフに追加し、プランナーへのエントリ ポイントを設定し、ノード間のエッジを定義します。\n",
    "\n",
    "ここで重要な部分は、生成ノードの後の条件付きエッジです。これは、should_continue 関数を使用して、反映して修正するか、プロセスを終了するかを決定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e15a20-83d7-434c-8551-bce8dcc32be0",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab2c74-f32e-490c-a85d-932d11444210",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833d3ce-bd31-4319-811d-decff226b970",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e93cce-6eab-4c7c-ac64-e9993fdb30d6",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\", should_continue, {END: END, \"reflect\": \"reflect\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d0990-a932-423f-9ff3-5cada58c5f32",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cde654-64e2-48bc-80a9-0ed668ccb7dc",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871f644-b131-4065-b7ce-b82c20a41f11",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cfcff",
   "metadata": {},
   "source": [
    "## グラフの実行\n",
    "\n",
    "エッセイ ライターをテストするために、graph.stream メソッドを使用しています。これにより、プロセスの各ステップをその発生時に確認できます。LangChain と LangSmith の違いに関するエッセイを、最大 2 回の修正で作成するように要求しています。\n",
    "\n",
    "実行中に、各ノードからの出力が表示され、エッセイが計画、調査、執筆、修正の各段階を通じてどのように進化するかがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f3be1d-cc4c-41fa-9863-3e386e88e305",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"task\": \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    },\n",
    "    thread,\n",
    "):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1664b5-75e0-46b7-9c2b-4ac9171f4597",
   "metadata": {},
   "source": [
    "## エッセイ ライター インターフェース\n",
    "\n",
    "最後に、Gradio を使用したシンプルな GUI を使用して、エッセイ ライターとのやり取りを簡単にします。\n",
    "\n",
    "**重要な注意**: Amazon SageMaker コード エディター内で Gradio を使用するには、アプリを `shared=True` モードで起動する必要があります。これにより、パブリック リンクが作成されます。[セキュリティとファイル アクセス](https://www.gradio.app/guides/sharing-your-app#security-and-file-access) を確認して、セキュリティへの影響を理解してください。\n",
    "\n",
    "この GUI を使用すると、エッセイのトピックを入力し、エッセイを生成し、プロセスの各ステップの結果を確認できます。また、各ステップの後にプロセスを中断したり、エッセイの現在の状態を確認したり、エッセイを別の方向に導きたい場合はトピックやプランを変更したりすることもできます。\n",
    "\n",
    "この GUI を使用すると、エッセイ ライターを簡単に試して、入力やプロセスの変更が最終出力にどのように影響するかを確認できます。\n",
    "\n",
    "これで、AI エッセイ ライター プロジェクトは終了です。これで、幅広いトピックに関するエッセイを調査、執筆、改良できる、複雑で多段階の AI エージェントが完成しました。このプロジェクトでは、さまざまな AI および API サービスを組み合わせて、強力で実用的なアプリケーションを作成する方法を説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad8a6cc-65d4-4ce7-87aa-4e67d7c23d7b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#set magic variables to allow for a reload when changing code without restarting the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gradio as gr\n",
    "from helper import ewriter, writer_gui\n",
    "\n",
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37002eb0",
   "metadata": {},
   "source": [
    "## 演習 - エッセイライターのプロンプトとパーサーを書き直す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b56d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e21704cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-dev-env",
   "language": "python",
   "name": "agents-dev-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
